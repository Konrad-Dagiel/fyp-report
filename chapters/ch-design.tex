\chapter{Design}

\section{Introduction to Dynamic Programming}
Dynamic Programming has many definitions, but can be summarized as method of breaking down a larger problem into sub-problems, so that if you work through the sub-problems in the right order, building each answer on the previous one, you eventually solve the larger problem.
The two attributes a problem needs to have in order to be classified as a dynamic programming problem are as follows:

\begin{definition}[Optimal Substructure]
    A problem is said to have optimal substructure if an optimal solution to the problem can be deduced from optimal solutions of some or all of its subproblems.
\end{definition}

\begin{definition}[Overlapping Subproblems]
    A problem is said to have overlapping subproblems if the problem can be broken down into subproblems which can be reused several times or a recursive algorithm would solve the same subproblem more than once resulting in repeated work. (If the subproblems do not overlap, the algorithm is categorized as a "divide and conquer" algorithm rather than a dynamic programming algorithm.)
\end{definition} 
Once we have deduced that a problem has both of these properties, we can use dynamic programming principles in order to solve the problem in an efficient manner.
When solving a dynamic programming problem, it is common to start by implementing a brute force solution which explores all subproblems and returns a solution.
We can then extend our solution to use a cache to store the results of any subproblems encountered, such that when the subproblem is encountered again we do not need to re-compute the result, instead we can simply look up the cache in constant time.
This is known as "memoization", or "top-down dynamic programming".
We can then look for any patterns in the cache table which, given an initialization (usually the base case of the recursive solution), would allow us to compute the values stored in the cache without ever traversing the decision tree of the problem itself.
This is known as "tabulation", or "bottom-up dynamic programming".
In order to demonstrate this, we will use a simple problem called The Fibonacci Problem.

\subsection{Demonstration of Dynamic Programming Principles using the Fibonacci Problem}
\input{chapters/algorithms/al-fibonacci.tex}

\section{Dynamic Programming Summary}

In summary, the "dynamic programming way of thinking" involves:
\begin{enumerate}
    \item Creating a brute force solution.
    \item Figuring out if the optimal substructure property holds.
    \item Identifying the repeating and overlapping subproblems.
    \item Introducing memoization to the brute force solution to eliminate repeated work.
    \item Using tabulation to try to deduce the memoization table bottom-up rather than top-down.
    \item Looking for ways to optimize space in the tabulation approach by reducing the size of the table.
\end{enumerate}

Using the Fibonacci example, we have demonstrated the way of thinking about a problem which is dynamic programming.
We have went from an $O(2^n)$ time and $O(n)$ space complexity recursive solution to an $O(n)$ time and $O(1)$ space complexity solution using dynamic programming principles.
The Project Notebook contains an in depth analysis of 8 well known dynamic programming problems, followed by my own original dynamic programming problems and solutions.
Where applicable, the problems analyzed contain:

\begin{enumerate}
    \item The problem statement, and a deep explanation of the problem with examples. This may contain example greedy algorithms and proofs of why they do not actually work for the given problem.
    \item A comprehensive brute force algorithm, with an implementation and complexity analysis.
    \item A short informal proof of why the optimal substructure and overlapping subproblems attributes hold.
    \item An explanation of how memoization is used in the problem, with an implementation and complexity analysis.
    \item An explanation of how tabulation is used in the problem, with an implementation and complexity analysis.
    \item An explanation of how we can space optimize the tabulation solution, with an implementation and complexity analysis.

\end{enumerate}

In the Project Notebook, the tabulation approach of each of the problems comes with a $printTable$ flag which, when set to $True$, displays the table which has been calculated for the specific problem.

\section{The Coin Change Problem}
\input{chapters/algorithms/al-coin_change.tex}

\section{Longest Increasing Subsequence}
\input{chapters/algorithms/al-lis.tex}

\section{Max Subarray Sum}
\input{chapters/algorithms/al-max_subarray_sum.tex}

\section{Longest Alternating Subsequence}
\input{chapters/algorithms/al-las.tex}

\section{Binomial Coefficients}
\input{chapters/algorithms/al-binomial_coefficients.tex}

\section{Longest Common Subsequence} \label{section:lcs}
\input{chapters/algorithms/al-lcs.tex}

\section{Longest Palindromic Subsequence}
\input{chapters/algorithms/al-lps.tex}

\section{Longest Contiguous Palindromic Substring}
\input{chapters/algorithms/al-lcps.tex}

\section{The Needleman-Wunsch Algorithm}
\input{chapters/algorithms/al-needleman_wunsch.tex}

\section{The Smith-Waterman Algorithm}

