\chapter{Research on Dynamic Programming}\label{chapter:research}
This chapter presents the outcomes of my research on dynamic programming as a topic, as well as detailed walkthroughs of eleven famous dynamic programming problems. These are also used as teaching material in the online guide.
The sources for this research are presented as sweeping citations in Section \ref{sec:sources}.

\section{Introduction to Dynamic Programming}
Dynamic Programming has many definitions, but can be summarized as a method of breaking down a larger problem into sub-problems, such that if you work through the sub-problems in the right order, building each answer on the previous one, you eventually arrive at a solution to the larger problem \cite{conversableeconomist}.
The two attributes a problem needs to have in order to be classified as a dynamic programming problem are as follows:

\begin{definition}[Optimal Substructure]
    A problem is said to have optimal substructure if an optimal solution to the problem can be deduced from optimal solutions of some or all of its subproblems.
\end{definition}

\begin{definition}[Overlapping Subproblems]
    A problem is said to have overlapping subproblems if the problem can be broken down into subproblems which can be reused several times, or a recursive algorithm would solve the same subproblem more than once resulting in repeated work. (If the subproblems do not overlap, the algorithm is categorized as a ``divide and conquer'' algorithm rather than a dynamic programming algorithm.)
\end{definition} 
Once we have deduced that a problem has both of these properties, we can use dynamic programming principles in order to solve the problem in an efficient manner.
When solving a dynamic programming problem, it is common to start by implementing a brute force solution which explores all subproblems and returns a solution.
We can then extend our solution to use a cache to store the results of any subproblems encountered, such that when the subproblem is encountered again we do not need to re-compute the result, instead we can simply look up the cache in constant time.
This is known as \textbf{memoization}, or ``top-down dynamic programming''.
We can then look for any patterns in the cache table which, given an initialization (usually the base case of the recursive solution), would allow us to compute the values stored in the cache without ever traversing the decision tree of the problem itself.
This is known as \textbf{tabulation}, or ``bottom-up dynamic programming''.
Finally, we can space optimize the cache to avoid storing information which is not needed to arrive at the solution.
This way, we arrive at the optimized dynamic programming solution to the problem.
In order to demonstrate this, we will use a simple problem called The Fibonacci Problem.

\section{The Fibonacci Problem - Fib(n)}
\input{chapters/algorithms/al-fibonacci.tex}

\section{Dynamic Programming Summary}

In summary, the ``dynamic programming way of thinking'' involves:
\begin{enumerate}
    \item Creating a brute force solution.
    \item Figuring out if the optimal substructure property holds.
    \item Identifying the repeating and overlapping subproblems.
    \item Introducing memoization to the brute force solution to eliminate repeated work.
    \item Using tabulation to try to deduce the memoization table bottom-up rather than top-down.
    \item Looking for ways to optimize space in the tabulation approach by reducing the size of the table.
\end{enumerate}

Using the Fibonacci example, we have demonstrated the dynamic programming way of thinking about a problem.
We have went from an $O(2^n)$ time and $O(n)$ space complexity recursive solution to an $O(n)$ time and $O(1)$ space complexity solution using dynamic programming principles.
The following section is an in depth analysis of eleven well known dynamic programming problems.
Where applicable, the problems analyzed contain:

\begin{enumerate}
    \item The problem statement, and a deep dive into what the problem is asking with examples. This may contain example greedy algorithms and proofs of why they are incorrect or sub-optimal for the given problem.
    \item A comprehensive brute force algorithm, with an implementation and complexity analysis.
    \item A short informal proof of why the optimal substructure and overlapping subproblems attributes hold.
    \item An explanation of how memoization is used in the problem, with an implementation and complexity analysis.
    \item An explanation of how tabulation is used in the problem, with an implementation and complexity analysis.
    \item An explanation of how we can space optimize the tabulation solution, with an implementation and complexity analysis.

\end{enumerate}

In the accompanying online guide, the tabulation approach of each of the problems comes with a $printTable$ flag which, when set to $True$, displays the table which has been calculated for the specific problem.

\section{The Coin Change Problem}
\input{chapters/algorithms/al-coin_change.tex}

\section{Longest Increasing Subsequence - LIS}
\input{chapters/algorithms/al-lis.tex}

\section{Max Subarray Sum - MSS}
\input{chapters/algorithms/al-max_subarray_sum.tex}

\section{Longest Alternating Subsequence - LAS}
\input{chapters/algorithms/al-las.tex}

\section{Binomial Coefficients - C(n,k)}
\input{chapters/algorithms/al-binomial_coefficients.tex}

\section{Longest Common Subsequence - LCS} \label{section:lcs}
\input{chapters/algorithms/al-lcs.tex}

\section{Longest Palindromic Subsequence - LPS}
\input{chapters/algorithms/al-lps.tex}

\section{Longest Contiguous Palindromic Substring - LPCS}
\input{chapters/algorithms/al-lcps.tex}

\section{The Needleman-Wunsch Algorithm}
\input{chapters/algorithms/al-needleman_wunsch.tex}

\section{The Smith-Waterman Algorithm}
\input{chapters/algorithms/al-smith_waterman.tex}

\section{Sources}\label{sec:sources}
This section outlines the sources used as research material for Chapter \ref{chapter:research}.
\begin{itemize}
    \item Richard Bellman's 1956 paper on the theory of dynamic programming \cite{bellman1954theory}.
    \item Introduction to Algorithms 2nd Edition Book \cite{algosbook1}.
    \item Introduction to Algorithms 3rd Edition Book \cite{algosbook2}.
    \item Conversable Economist Blog \cite{conversableeconomist}.
    \item Richard Bellman's 1966 paper on dynamic programming \cite{bellman1966dynamic}.
    \item Leetcode.com \cite{leetcode}.
    \item EnjoyAlgorithms.com \cite{enjoyalgorithms}.
    \item GeeksForGeeks.com \cite{geeksforgeeks}
    \item University of Melbourne Bi021 Lecture on sequence alignment \cite{likic2008needleman}.
    \item Neissery Abdulla paper on dynamic programming \cite{abdulla2018dynamic}.
    \item Optimization Tools for Logistics Book \cite{REVEILLAC201555}.
\end{itemize}


