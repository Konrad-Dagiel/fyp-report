\chapter{Benchmarking of the Researched Algorithms}
This section presents a comparative analysis of dynamic programming algorithms,
focusing on their computational efficiency.
Using Python's timeit module,
we conduct timed experiments to measure the execution performance of these algorithms across varying input sizes and scenarios.
We aim to evaluate the scalability and practical effectiveness of these algorithms in solving real-world computational problems. 

\subsection*{The Timeit Module}
This module provides a simple way to time small bits of Python code.
It has both a Command-Line Interface as well as a callable one.
It avoids a number of common traps for measuring execution times.
See also Tim Peters’ introduction to the “Algorithms” chapter in the second edition of Python Cookbook,
published by O’Reilly.\\
The following are some of the common traps for measuring execution time that timeit avoids:
\begin{itemize}
    \item Warm-up Time: The Timeit module runs the code multiple times (by default, 1 million times) to ensure that any overhead related to JIT compilation or other one-time operations is amortized and doesn't skew the timing results.
    This helps ensure that the measured time reflects the actual performance of the code snippet, rather than the overhead of the Python interpreter starting up or other initialization tasks.

    \item External Factors: Timeit executes the code in a controlled environment, minimizing the impact of external factors such as other processes running on the system, I/O operations, or system load. This helps ensure that the timing measurements are as consistent and reliable as possible.

    \item Garbage Collection: The Timeit module disables the garbage collector during timing measurements to prevent it from interfering with the results. Garbage collection can introduce variability in execution times, especially for short code snippets, so disabling it helps ensure more consistent timing measurements.

    \item Timer Resolution: Timeit uses high-resolution timers to measure execution times accurately, taking into account the system's timer resolution. This helps avoid inaccuracies that can arise from low-resolution timers, especially when measuring very short code snippets.

    \item Compilation: For code snippets that are executed multiple times, timeit compiles the code to bytecode to speed up execution. This compilation step is done automatically and transparently, helping to reduce overhead and improve the accuracy of timing measurements.
\end{itemize}

\subsection{Discussion of Benchmarking Methods of Algorithms with Inputs of Size N}
When benchmarking algorithms with a variable input size, we cannot simply run the code multiple times on the same fixed input,
as we cannot be sure if this input accurately reflects the average runtime of the algorithm.
Instead, on each run, the input size is fixed but the input itself is randomized.
If we do this enough times, we should converge on the actual average runtime of the algorithm.
To help demonstrate this point, we can look at the worst case runtime of Quicksort $O(n^2)$, compared to its average case runtime $O(nlogn)$.
If we were to test Quicksort vs Mergesort on a list that's sorted in reverse order, Mergesort would almost certainly come out on top, which is not indicative of the actual average runtimes of the algorithms.
Therefore, we must somehow randomize the input on each test run, and get an average performance instead.
When benchmarking the researched algorithms, we will use the following approaches in an attempt to get a more accurate view of the actual average runtimes on random examples.


\subsection{Approach to Benchmarking the Fibonacci Problem}

In order to benchmark the Fibonacci Problem, we do not need any randomization. We can simply input $n$, the index of the fibonacci number to be computed, and calculate the amount of time each of the brute force, memoization and tabulation approaches take to solve for $fib(n)$.
The code used for benchmarking the different implementations of this problem is shown in Figure \ref{fig:bm-fibonacci}.

\begin{figure}[H]
    \centering
    \begin{lstlisting}
    def benchmark_fib(funcs, n, repetitions):
        res = [-1] * len(funcs)
        res_idx = 0
        for func in funcs:
            total_time = 0
            for _ in range(repetitions):
    
                execution_time = timeit.timeit(lambda: func(n), number=1000)
    
                total_time += execution_time
    
            average_time = total_time / repetitions
            res[res_idx] = average_time
            res_idx += 1
        return res
    \end{lstlisting}
    \caption{Python Code Used for Benchmarking the Fibonacci Implementations}
    \label{fig:bm-fibonacci}
\end{figure}

\subsection{Results of Benchmarking the Fibonacci Problem}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{n=}  & \textbf{5} & \textbf{10} & \textbf{25} & \textbf{35} & \textbf{50}  & \textbf{1000} \\
        \hline
        \textbf{Brute Force} & 0.000092 & 0.012308 & 1.365216 & 145.007681 & * & * \\
        \hline
        \textbf{Memoization} & 0.000218 & 0.000154 & 0.000231 & 0.000238 & 0.000275 & 0.000303 \\
        \hline
        \textbf{Tabulation} & 0.000867 & 0.001471 & 0.003473 & 0.004803 & 0.005291 & 0.124944 \\
        \hline
        \textbf{Optimized} & 0.000519 & 0.000534 & 0.001210 & 0.001620 & 0.002763 & 0.053696 \\
        \hline
    \end{tabular}
    \caption{Results of Benchmarking the Fibonacci Problem}
\end{table}

*=over 10 mins to run on an MSI GF63 SCXR.\\
Results are rounded to 6 decimal places.


\subsection{Approach to Benchmarking the Coin Change Problem}

In order to benchmark the Coin Change Problem, we must randomize a list of coin denominations $D$ of fixed size, and a total amount $a$, in order to get an accurate insight into the average runtime.
The maximum and minimum size of the coin denominations and the total amount should be kept consistent throughout test runs to reduce the amount of variance.
Ideally, the inputs to each of the brute force, memoization and tabulation algorithms should be the same on each run to avoid the case where one of the algorithms "gets lucky" and consistently gets "easier" inputs than the others.
For simplicity, we se the target amount $a$ to the length of $D$.

NOTE: Make sure $D\_min$ is NOT 0, as having a zero size coin will make the brute force and memoization solution non-terminable.
The code used for benchmarking the different implementations of this problem is shown in Figure \ref{fig:bm-coin-change}.

\begin{figure}[H]
    \centering
    \begin{lstlisting}
    def benchmark_coin_change(funcs, D_len, D_min, D_max, a_min, a_max, repetitions):
        res = [-1] * len(funcs)
        res_idx = 0
        for func in funcs:
            total_time = 0
            for _ in range(repetitions):
    
                random_D = [random.randint(D_min, D_max) for _ in range(D_len)]
    
                random_a = random.randint(a_min, a_max)
                
                execution_time = timeit.timeit(lambda: func(random_D, random_a), number=1)
    
                total_time += execution_time
    
            average_time = total_time / repetitions
            res[res_idx] = average_time
            res_idx += 1
        return res
    \end{lstlisting}
    \caption{Python Code Used for Benchmarking the Coin Change Implementations}
    \label{fig:bm-coin-change}
\end{figure}

\subsection{Results of Benchmarking the Coin Change Problem}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{|D|=}  & \textbf{3} & \textbf{5} & \textbf{10} & \textbf{15} & \textbf{17}  & \textbf{1000} \\
        \hline
        \textbf{Brute Force} & 0.000026 & 0.000129 & 0.002109 & 2.391904 & * & * \\
        \hline
        \textbf{Memoization} & 0.000013 & 0.000024 & 0.000070 & 0.000040 & 2.391904 & 0.329376 \\
        \hline
        \textbf{Tabulation} & 0.000017 & 0.000012 & 0.000033 & 0.000016 & 0.000060 & 0.243787 \\
        \hline
    \end{tabular}
    \caption{Results of Benchmarking the Coin Change Problem}
\end{table}
*=over 10 mins to run on an MSI GF63 Thin SCXR.\\
Results in seconds rounded to 6 decimal places.


\subsection{Approach to Benchmarking Problems With Single List Inputs}
In order to benchmark problems with single list inputs, we must randomize a list nums of fixed size in order to get an accurate insight into the average runtime.
The maximum and minimum size of the elements of $nums$ should be kept consistent throughout test runs to reduce the amount of variance.
Ideally, the inputs to each of the brute force, memoization and tabulation algorithms should be the same on each run to avoid the case where one of the algorithms "gets lucky" and consistently gets "easier" inputs than the others.
For simplicity, the max number size in $nums$ is set to $|nums|$, and the minimum is set to $1$ or $-1 * |nums|$ (depending on the constraints of the problem).
The code used for benchmarking the different implementations of problems of this type is shown in Figure \ref{fig:bm-subsequence}.

\begin{figure}[H]
    \centering
    \begin{lstlisting}
    def benchmark_subsequence(funcs, nums_len, nums_min, nums_max, repetitions):
        res = [-1] * len(funcs)
        res_idx = 0
        for func in funcs:
            total_time = 0
            for _ in range(repetitions):
    
                random_nums = [random.randint(nums_min, nums_max) for _ in range(nums_len)]
                
                execution_time = timeit.timeit(lambda: func(random_nums), number=1)
    
                total_time += execution_time
    
            average_time = total_time / repetitions
            res[res_idx] = average_time
            res_idx += 1
        return res
    \end{lstlisting}
    \caption{Python Code Used for Benchmarking Subsequence Problem Implementations}
    \label{fig:bm-subsequence}
\end{figure}

\subsection{Results of Benchmarking Longest Increasing Subsequence}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{|nums|=}  & \textbf{5} & \textbf{10} & \textbf{25} & \textbf{50} & \textbf{100}  & \textbf{1000} \\
        \hline
        \textbf{Brute Force} & 0.000008 & 0.000071 & 0.001841 & 0.118709 & 53.019671 & * \\
        \hline
        \textbf{Memoization} & 0.000012 & 0.000040 & 0.000182 & 0.000714 & 0.002965 & 0.461804 \\
        \hline
        \textbf{Tabulation} & 0.000006 & 0.000011 & 0.000043 & 0.000162 & 0.000664 & 0.059541 \\
        \hline
    \end{tabular}
    \caption{Results of Benchmarking Longest Increasing Subsequence}
\end{table}
*=over 10 mins to run on an MSI GF63 Thin SCXR.\\
Results in seconds rounded to 6 decimal places.

\subsection{Results of Benchmarking Max Subarray Sum}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{|nums|=}  & \textbf{5} & \textbf{25} & \textbf{100} & \textbf{1000} & \textbf{10000}  & \textbf{1000000} \\
        \hline
        \textbf{Brute Force} & 0.000007 & 0.000071 & 0.001073 & 0.076034 & 7.504696 & * \\
        \hline
        \textbf{Kadanes} & 0.000002 & 0.000005 & 0.000027 & 0.000158 & 0.001591 & 0.016662 \\
        \hline
    \end{tabular}
    \caption{Results of Benchmarking Max Subarray Sum}
\end{table}
*=over 10 mins to run on an MSI GF63 Thin SCXR.\\
Results in seconds rounded to 6 decimal places.

\subsection{Results of Benchmarking Longest Alternating Subsequence}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{|nums|=}  & \textbf{10} & \textbf{20} & \textbf{30} & \textbf{50} & \textbf{1000}  & \textbf{10000} \\
        \hline
        \textbf{Brute Force} & 0.001798 & 2.723978 & * & * & * & * \\
        \hline
        \textbf{Auxiliary} & 0.000013 & 0.000043 & 0.000146 & 0.000379 & 0.106903 & 11.012398 \\
        \hline
        \textbf{Optimized} &  0.000003 & 0.000003 & 0.000008 & 0.000018 & 0.000106 & 0.001133 \\
        \hline
    \end{tabular}
    \caption{Results of Benchmarking Longest Alternating Subsequence}
\end{table}
*=over 10 mins to run on an MSI GF63 Thin SCXR.\\
Results in seconds rounded to 6 decimal places.


\subsection{Approach to Benchmarking the Binomial Coefficients Problem}
In order to benchmark the Binomial Coefficients Problem, we do not need any randomization. We can simply input $n$ and $k$, and calculate the average time each of the brute force, memoization and tabulation approaches take to solve for $C(n,k)$.
For simplicity, we take $k$ as $n/2$.
The code used for benchmarking the different implementations of this problem is shown in Figure \ref{fig:bm-binomial}.

\begin{figure}[H]
    \centering
    \begin{lstlisting}
    def benchmark_binomial_coefficients(funcs, n,k, repetitions):
        res = [-1] * len(funcs)
        res_idx = 0
        for func in funcs:
            total_time = 0
            for _ in range(repetitions):
    
                execution_time = timeit.timeit(lambda: func(n,k), number=1)
    
                total_time += execution_time
    
            average_time = total_time / repetitions
            res[res_idx] = average_time
            res_idx += 1
        return res
    \end{lstlisting}
    \caption{Python Code Used for Benchmarking the Binomial Coefficients Implementations}
    \label{fig:bm-binomial}
\end{figure}

\subsection{Results of Benchmarking Binomial Coefficients}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{n=}  & \textbf{10} & \textbf{20} & \textbf{50} & \textbf{1000} & \textbf{10000} \\
        \hline
        \textbf{Brute Force} & 0.000056 & 0.034847 & * & * & + \\
        \hline
        \textbf{Memoization} & 0.000001 & 0.000001 & 0.000061 & 0.020484 & + \\
        \hline
        \textbf{Tabulation} & 0.000013 & 0.000030 & 0.000212 & 0.078734 & + \\
        \hline
        \textbf{Optimized} & 0.000015 & 0.000020 & 0.000118 & 0.033395 & + \\
        \hline
    \end{tabular}
    \caption{Results of Benchmarking Binomial Coefficients}
\end{table}
*=over 10 mins to run on an MSI GF63 Thin SCXR.\\
+=maximum recursion depth reached/crash.\\
Results in seconds rounded to 6 decimal places.